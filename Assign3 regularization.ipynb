{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f, encoding='latin1')\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Logistic Regression with L2 Regularization\n",
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_data = tf.placeholder(tf.float32, shape=(batch_size, image_size*image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    \n",
    "    tf_valid_data = tf.constant(valid_dataset)\n",
    "    tf_test_data = tf.constant(test_dataset)\n",
    "    \n",
    "    beta = tf.placeholder(tf.float32)\n",
    "    \n",
    "    #random weights and zeros bias at start\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size*image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    logits = tf.matmul(tf_train_data, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=tf_train_labels) + beta*(tf.nn.l2_loss(weights)))\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    train_preds = tf.nn.softmax(logits)\n",
    "    valid_preds = tf.nn.softmax(tf.matmul(tf_valid_data, weights) + biases)\n",
    "    test_preds = tf.nn.softmax(tf.matmul(tf_test_data, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.00000000e-05   1.11200000e-02   2.22300000e-02   3.33400000e-02\n",
      "   4.44500000e-02   5.55600000e-02   6.66700000e-02   7.77800000e-02\n",
      "   8.88900000e-02   1.00000000e-01]\n",
      "=============\n",
      "progress: 10.0%\n",
      "Minibatch loss at step 0: 4335.184082\n",
      "Minibatch accuracy: 3.9%\n",
      "Minibatch loss at step 500: 36.099014\n",
      "Minibatch accuracy: 64.8%\n",
      "Minibatch loss at step 1000: 11.942572\n",
      "Minibatch accuracy: 66.4%\n",
      "Minibatch loss at step 1500: 9.199468\n",
      "Minibatch accuracy: 71.9%\n",
      "Minibatch loss at step 2000: 10.396114\n",
      "Minibatch accuracy: 70.3%\n",
      "Minibatch loss at step 2500: 6.658521\n",
      "Minibatch accuracy: 60.2%\n",
      "Minibatch loss at step 3000: 6.670716\n",
      "Minibatch accuracy: 67.2%\n",
      "=============\n",
      "progress: 20.0%\n",
      "Minibatch loss at step 0: 7710.280762\n",
      "Minibatch accuracy: 6.2%\n",
      "Minibatch loss at step 500: 14.357397\n",
      "Minibatch accuracy: 78.1%\n",
      "Minibatch loss at step 1000: 1.840452\n",
      "Minibatch accuracy: 68.0%\n",
      "Minibatch loss at step 1500: 1.598415\n",
      "Minibatch accuracy: 69.5%\n",
      "Minibatch loss at step 2000: 1.874321\n",
      "Minibatch accuracy: 64.1%\n",
      "Minibatch loss at step 2500: 1.644741\n",
      "Minibatch accuracy: 76.6%\n",
      "Minibatch loss at step 3000: 1.974257\n",
      "Minibatch accuracy: 64.1%\n",
      "=============\n",
      "progress: 30.0%\n",
      "Minibatch loss at step 0: 11202.942383\n",
      "Minibatch accuracy: 7.0%\n",
      "Minibatch loss at step 500: 1.849483\n",
      "Minibatch accuracy: 80.5%\n",
      "Minibatch loss at step 1000: 1.855677\n",
      "Minibatch accuracy: 66.4%\n",
      "Minibatch loss at step 1500: 1.735246\n",
      "Minibatch accuracy: 71.1%\n",
      "Minibatch loss at step 2000: 2.120267\n",
      "Minibatch accuracy: 61.7%\n",
      "Minibatch loss at step 2500: 1.769253\n",
      "Minibatch accuracy: 74.2%\n",
      "Minibatch loss at step 3000: 2.031357\n",
      "Minibatch accuracy: 64.8%\n",
      "=============\n",
      "progress: 40.0%\n",
      "Minibatch loss at step 0: 14719.109375\n",
      "Minibatch accuracy: 10.2%\n",
      "Minibatch loss at step 500: 1.853506\n",
      "Minibatch accuracy: 77.3%\n",
      "Minibatch loss at step 1000: 1.925553\n",
      "Minibatch accuracy: 61.7%\n",
      "Minibatch loss at step 1500: 1.819226\n",
      "Minibatch accuracy: 66.4%\n",
      "Minibatch loss at step 2000: 2.141699\n",
      "Minibatch accuracy: 58.6%\n",
      "Minibatch loss at step 2500: 1.858231\n",
      "Minibatch accuracy: 72.7%\n",
      "Minibatch loss at step 3000: 2.065327\n",
      "Minibatch accuracy: 59.4%\n",
      "=============\n",
      "progress: 50.0%\n",
      "Minibatch loss at step 0: 18063.078125\n",
      "Minibatch accuracy: 14.1%\n",
      "Minibatch loss at step 500: 1.935588\n",
      "Minibatch accuracy: 68.8%\n",
      "Minibatch loss at step 1000: 1.991074\n",
      "Minibatch accuracy: 58.6%\n",
      "Minibatch loss at step 1500: 1.885803\n",
      "Minibatch accuracy: 60.2%\n",
      "Minibatch loss at step 2000: 2.136066\n",
      "Minibatch accuracy: 56.2%\n",
      "Minibatch loss at step 2500: 1.925132\n",
      "Minibatch accuracy: 70.3%\n",
      "Minibatch loss at step 3000: 2.085033\n",
      "Minibatch accuracy: 54.7%\n",
      "=============\n",
      "progress: 60.0%\n",
      "Minibatch loss at step 0: 21502.230469\n",
      "Minibatch accuracy: 10.2%\n",
      "Minibatch loss at step 500: 1.995428\n",
      "Minibatch accuracy: 66.4%\n",
      "Minibatch loss at step 1000: 2.034457\n",
      "Minibatch accuracy: 56.2%\n",
      "Minibatch loss at step 1500: 1.934098\n",
      "Minibatch accuracy: 53.9%\n",
      "Minibatch loss at step 2000: 2.137671\n",
      "Minibatch accuracy: 56.2%\n",
      "Minibatch loss at step 2500: 1.978028\n",
      "Minibatch accuracy: 66.4%\n",
      "Minibatch loss at step 3000: 2.092150\n",
      "Minibatch accuracy: 51.6%\n",
      "=============\n",
      "progress: 70.0%\n",
      "Minibatch loss at step 0: 25022.640625\n",
      "Minibatch accuracy: 14.8%\n",
      "Minibatch loss at step 500: 2.039496\n",
      "Minibatch accuracy: 60.2%\n",
      "Minibatch loss at step 1000: 2.073797\n",
      "Minibatch accuracy: 53.9%\n",
      "Minibatch loss at step 1500: 1.972789\n",
      "Minibatch accuracy: 48.4%\n",
      "Minibatch loss at step 2000: 2.145654\n",
      "Minibatch accuracy: 55.5%\n",
      "Minibatch loss at step 2500: 2.022589\n",
      "Minibatch accuracy: 62.5%\n",
      "Minibatch loss at step 3000: 2.111984\n",
      "Minibatch accuracy: 50.0%\n",
      "=============\n",
      "progress: 80.0%\n",
      "Minibatch loss at step 0: 28484.273438\n",
      "Minibatch accuracy: 5.5%\n",
      "Minibatch loss at step 500: 2.071609\n",
      "Minibatch accuracy: 53.1%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-740b3771bab7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m                        beta : reg}\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_preds\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m500\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Minibatch loss at step %d: %f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "reg_vals = np.linspace(0.00001, 0.1, 10)\n",
    "print(reg_vals)\n",
    "acc_train = []\n",
    "acc_valid = []\n",
    "acc_test = []\n",
    "i=0\n",
    "for reg in reg_vals:\n",
    "    i+=1\n",
    "    print(\"=============\\nprogress: %.1f%%\" % (100*i/len(reg_vals)))\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        for step in range(num_steps):\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            \n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            \n",
    "            tf_dict = {tf_train_data : batch_data, \n",
    "                       tf_train_labels : batch_labels, \n",
    "                       beta : reg}\n",
    "            \n",
    "            _, l, predictions = session.run([optimizer, loss, train_preds], feed_dict = tf_dict)\n",
    "            if (step % 500 == 0):\n",
    "                print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "                print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "        acc_valid.append(accuracy(valid_preds.eval(), valid_labels))\n",
    "        acc_test.append(accuracy(test_preds.eval(), test_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (16,9))\n",
    "plt.semilogx(reg_vals, acc_valid)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')\n",
    "\n",
    "fig = plt.figure(figsize = (16,9))\n",
    "plt.semilogx(reg_vals, acc_test)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "1 Layer NN L2 Regularization\n",
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_nodes= 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_data = tf.placeholder(tf.float32,shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_data = tf.constant(valid_dataset)\n",
    "    tf_test_data = tf.constant(test_dataset)\n",
    "    \n",
    "    beta = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Variables.\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([num_nodes]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([num_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Training computation.\n",
    "    layer1 = tf.nn.relu(tf.matmul(tf_train_data, weights1) + biases1)\n",
    "    logit2 = tf.matmul(layer1, weights2) + biases2\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logit2)\n",
    "        + beta*(tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2)))\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_preds = tf.nn.softmax(logit2)\n",
    "    \n",
    "    layer1 = tf.nn.relu(tf.matmul(tf_test_data, weights1) + biases1)\n",
    "    test_preds = tf.nn.softmax(tf.matmul(layer1, weights2) + biases2)\n",
    "    \n",
    "    layer1 = tf.nn.relu(tf.matmul(tf_valid_data, weights1) + biases1)\n",
    "    valid_preds = tf.nn.softmax(tf.matmul(layer1, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.00000000e-05   1.11200000e-02   2.22300000e-02   3.33400000e-02\n",
      "   4.44500000e-02   5.55600000e-02   6.66700000e-02   7.77800000e-02\n",
      "   8.88900000e-02   1.00000000e-01]\n",
      "=============\n",
      "progress: 10.0%\n",
      "Minibatch loss at step 0: 4320.321289\n",
      "Minibatch accuracy: 11.7%\n",
      "Minibatch loss at step 500: 29.803169\n",
      "Minibatch accuracy: 58.6%\n",
      "Minibatch loss at step 1000: 11.051771\n",
      "Minibatch accuracy: 66.4%\n",
      "Minibatch loss at step 1500: 11.007452\n",
      "Minibatch accuracy: 68.8%\n",
      "Minibatch loss at step 2000: 9.363455\n",
      "Minibatch accuracy: 71.1%\n",
      "Minibatch loss at step 2500: 6.738040\n",
      "Minibatch accuracy: 61.7%\n",
      "Minibatch loss at step 3000: 6.948409\n",
      "Minibatch accuracy: 67.2%\n",
      "=============\n",
      "progress: 20.0%\n",
      "Minibatch loss at step 0: 7828.930176\n",
      "Minibatch accuracy: 7.8%\n",
      "Minibatch loss at step 500: 14.383654\n",
      "Minibatch accuracy: 77.3%\n",
      "Minibatch loss at step 1000: 1.782471\n",
      "Minibatch accuracy: 71.1%\n",
      "Minibatch loss at step 1500: 1.599851\n",
      "Minibatch accuracy: 69.5%\n",
      "Minibatch loss at step 2000: 1.874439\n",
      "Minibatch accuracy: 64.1%\n",
      "Minibatch loss at step 2500: 1.641145\n",
      "Minibatch accuracy: 76.6%\n",
      "Minibatch loss at step 3000: 1.972456\n",
      "Minibatch accuracy: 64.1%\n",
      "=============\n",
      "progress: 30.0%\n",
      "Minibatch loss at step 0: 11165.646484\n",
      "Minibatch accuracy: 11.7%\n",
      "Minibatch loss at step 500: 1.844063\n",
      "Minibatch accuracy: 78.9%\n",
      "Minibatch loss at step 1000: 1.855684\n",
      "Minibatch accuracy: 66.4%\n",
      "Minibatch loss at step 1500: 1.736489\n",
      "Minibatch accuracy: 68.8%\n",
      "Minibatch loss at step 2000: 2.181293\n",
      "Minibatch accuracy: 60.2%\n",
      "Minibatch loss at step 2500: 1.768010\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 3000: 2.027228\n",
      "Minibatch accuracy: 65.6%\n",
      "=============\n",
      "progress: 40.0%\n",
      "Minibatch loss at step 0: 14647.419922\n",
      "Minibatch accuracy: 7.8%\n",
      "Minibatch loss at step 500: 1.850498\n",
      "Minibatch accuracy: 77.3%\n",
      "Minibatch loss at step 1000: 1.923623\n",
      "Minibatch accuracy: 61.7%\n",
      "Minibatch loss at step 1500: 1.815768\n",
      "Minibatch accuracy: 68.0%\n",
      "Minibatch loss at step 2000: 2.141231\n",
      "Minibatch accuracy: 58.6%\n",
      "Minibatch loss at step 2500: 1.859049\n",
      "Minibatch accuracy: 72.7%\n",
      "Minibatch loss at step 3000: 2.061107\n",
      "Minibatch accuracy: 60.2%\n",
      "=============\n",
      "progress: 50.0%\n",
      "Minibatch loss at step 0: 18181.158203\n",
      "Minibatch accuracy: 7.8%\n",
      "Minibatch loss at step 500: 1.936318\n",
      "Minibatch accuracy: 69.5%\n",
      "Minibatch loss at step 1000: 1.988899\n",
      "Minibatch accuracy: 58.6%\n",
      "Minibatch loss at step 1500: 1.883544\n",
      "Minibatch accuracy: 60.2%\n",
      "Minibatch loss at step 2000: 2.133531\n",
      "Minibatch accuracy: 56.2%\n",
      "Minibatch loss at step 2500: 1.926033\n",
      "Minibatch accuracy: 70.3%\n",
      "Minibatch loss at step 3000: 2.082502\n",
      "Minibatch accuracy: 54.7%\n",
      "=============\n",
      "progress: 60.0%\n",
      "Minibatch loss at step 0: 21603.564453\n",
      "Minibatch accuracy: 11.7%\n",
      "Minibatch loss at step 500: 1.999100\n",
      "Minibatch accuracy: 66.4%\n",
      "Minibatch loss at step 1000: 2.032894\n",
      "Minibatch accuracy: 55.5%\n",
      "Minibatch loss at step 1500: 1.934979\n",
      "Minibatch accuracy: 53.9%\n",
      "Minibatch loss at step 2000: 2.136950\n",
      "Minibatch accuracy: 57.8%\n",
      "Minibatch loss at step 2500: 1.978552\n",
      "Minibatch accuracy: 66.4%\n",
      "Minibatch loss at step 3000: 2.092538\n",
      "Minibatch accuracy: 50.0%\n",
      "=============\n",
      "progress: 70.0%\n",
      "Minibatch loss at step 0: 25113.412109\n",
      "Minibatch accuracy: 11.7%\n",
      "Minibatch loss at step 500: 2.041778\n",
      "Minibatch accuracy: 60.2%\n",
      "Minibatch loss at step 1000: 2.078013\n",
      "Minibatch accuracy: 53.9%\n",
      "Minibatch loss at step 1500: 1.975650\n",
      "Minibatch accuracy: 46.9%\n",
      "Minibatch loss at step 2000: 2.145610\n",
      "Minibatch accuracy: 55.5%\n",
      "Minibatch loss at step 2500: 2.022656\n",
      "Minibatch accuracy: 62.5%\n",
      "Minibatch loss at step 3000: 2.111617\n",
      "Minibatch accuracy: 50.0%\n",
      "=============\n",
      "progress: 80.0%\n",
      "Minibatch loss at step 0: 28515.718750\n",
      "Minibatch accuracy: 4.7%\n",
      "Minibatch loss at step 500: 2.073613\n",
      "Minibatch accuracy: 53.1%\n",
      "Minibatch loss at step 1000: 2.111279\n",
      "Minibatch accuracy: 53.1%\n",
      "Minibatch loss at step 1500: 1.999642\n",
      "Minibatch accuracy: 43.0%\n",
      "Minibatch loss at step 2000: 2.151417\n",
      "Minibatch accuracy: 54.7%\n",
      "Minibatch loss at step 2500: 2.055147\n",
      "Minibatch accuracy: 61.7%\n",
      "Minibatch loss at step 3000: 2.132347\n",
      "Minibatch accuracy: 48.4%\n",
      "=============\n",
      "progress: 90.0%\n",
      "Minibatch loss at step 0: 31917.810547\n",
      "Minibatch accuracy: 7.0%\n",
      "Minibatch loss at step 500: 2.102571\n",
      "Minibatch accuracy: 53.9%\n",
      "Minibatch loss at step 1000: 2.140520\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 1500: 2.026131\n",
      "Minibatch accuracy: 41.4%\n",
      "Minibatch loss at step 2000: 2.158342\n",
      "Minibatch accuracy: 54.7%\n",
      "Minibatch loss at step 2500: 2.084664\n",
      "Minibatch accuracy: 60.2%\n",
      "Minibatch loss at step 3000: 2.154578\n",
      "Minibatch accuracy: 48.4%\n",
      "=============\n",
      "progress: 100.0%\n",
      "Minibatch loss at step 0: 35418.234375\n",
      "Minibatch accuracy: 6.2%\n",
      "Minibatch loss at step 500: 2.125116\n",
      "Minibatch accuracy: 53.1%\n",
      "Minibatch loss at step 1000: 2.163275\n",
      "Minibatch accuracy: 46.9%\n",
      "Minibatch loss at step 1500: 2.046707\n",
      "Minibatch accuracy: 40.6%\n",
      "Minibatch loss at step 2000: 2.163831\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 2500: 2.111001\n",
      "Minibatch accuracy: 55.5%\n",
      "Minibatch loss at step 3000: 2.173497\n",
      "Minibatch accuracy: 49.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "reg_vals = np.linspace(0.00001, 0.1, 10)\n",
    "print(reg_vals)\n",
    "acc_train = []\n",
    "acc_valid = []\n",
    "acc_test = []\n",
    "i=0\n",
    "for reg in reg_vals:\n",
    "    i+=1\n",
    "    print(\"=============\\nprogress: %.1f%%\" % (100*i/len(reg_vals)))\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        for step in range(num_steps):\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            \n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            \n",
    "            tf_dict = {tf_train_data : batch_data, \n",
    "                       tf_train_labels : batch_labels, \n",
    "                       beta : reg}\n",
    "            \n",
    "            _, l, predictions = session.run(\n",
    "                [optimizer, loss, train_preds], feed_dict = tf_dict)\n",
    "            \n",
    "            if (step % 500 == 0):\n",
    "                print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "                print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "        acc_valid.append(accuracy(valid_preds.eval(), valid_labels))\n",
    "        acc_test.append(accuracy(test_preds.eval(), test_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x26e27432630>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6IAAAIcCAYAAADsTxM2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVYooAAAArVJEAQAAaJUiCgAAQKv+H2wrIXohcnNBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26e274c2860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6IAAAIcCAYAAADsTxM2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26e274b09e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize = (16,9))\n",
    "plt.semilogx(reg_vals, acc_valid)\n",
    "plt.grid(True)\n",
    "plt.title('Validation accuracy by regularization')\n",
    "\n",
    "fig = plt.figure(figsize = (16,9))\n",
    "plt.semilogx(reg_vals, acc_test)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_nodes= 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_data = tf.placeholder(tf.float32,shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_data = tf.constant(valid_dataset)\n",
    "    tf_test_data = tf.constant(test_dataset)\n",
    "    \n",
    "    beta = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Variables.\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([num_nodes]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([num_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Training computation.\n",
    "    layer1 = tf.nn.relu(tf.matmul(tf_train_data, weights1) + biases1)\n",
    "    logit2 = tf.matmul(layer1, weights2) + biases2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf_train_labels, logits=logit2))\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_preds = tf.nn.softmax(logit2)\n",
    "    \n",
    "    layer1_test = tf.nn.relu(tf.matmul(tf_test_data, weights1) + biases1)\n",
    "    test_preds = tf.nn.softmax(tf.matmul(layer1_test, weights2) + biases2)\n",
    "    \n",
    "    layer1_valid = tf.nn.relu(tf.matmul(tf_valid_data, weights1) + biases1)\n",
    "    valid_preds = tf.nn.softmax(tf.matmul(layer1_valid, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 298.798309\n",
      "Minibatch accuracy: 7.8%\n",
      "Minibatch loss at step 10: 26.500786\n",
      "Minibatch accuracy: 90.6%\n",
      "Minibatch loss at step 20: 0.000008\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 30: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 40: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 50: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 60: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 70: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 80: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 90: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_batches = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(num_steps):\n",
    "        offset = ((step % num_batches) * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        tf_dict = {tf_train_data : batch_data, \n",
    "                   tf_train_labels : batch_labels, \n",
    "                   beta : 1e-3}\n",
    "        \n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_preds], feed_dict = tf_dict)\n",
    "        \n",
    "        if (step % 10 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_nodes= 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_data = tf.placeholder(tf.float32,shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_data = tf.constant(valid_dataset)\n",
    "    tf_test_data = tf.constant(test_dataset)\n",
    "    \n",
    "    beta = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Variables.\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([num_nodes]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([num_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Training computation.\n",
    "    layer1 = tf.nn.relu(tf.matmul(tf_train_data, weights1) + biases1)\n",
    "    drop_layer1 = tf.nn.dropout(layer1, 0.5)\n",
    "    logit2 = tf.matmul(drop_layer1, weights2) + biases2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf_train_labels, logits=logit2))\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_preds = tf.nn.softmax(logit2)\n",
    "    \n",
    "    layer1_test = tf.nn.relu(tf.matmul(tf_test_data, weights1) + biases1)\n",
    "    test_preds = tf.nn.softmax(tf.matmul(layer1_test, weights2) + biases2)\n",
    "    \n",
    "    layer1_valid = tf.nn.relu(tf.matmul(tf_valid_data, weights1) + biases1)\n",
    "    valid_preds = tf.nn.softmax(tf.matmul(layer1_valid, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 467.658264\n",
      "Minibatch accuracy: 13.3%\n",
      "Minibatch loss at step 100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 200: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 300: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 400: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 600: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 700: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 800: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 900: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 1000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 1100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 1200: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 1300: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 1400: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 1500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 1600: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 1700: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 1800: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 1900: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 2000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 2100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 2200: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 2300: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 2400: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 2500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 2600: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 2700: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 2800: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 2900: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 3000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(num_steps):\n",
    "        offset = (step % 5 )#* batch_size)  % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        tf_dict = {tf_train_data : batch_data, \n",
    "                   tf_train_labels : batch_labels, \n",
    "                   beta : 1e-3}\n",
    "        \n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_preds], feed_dict = tf_dict)\n",
    "        \n",
    "        if (step % 100 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Training a Deep Network\n",
    "---------\n",
    "Ok So this is the network i talked baout in the meeting.\n",
    "\n",
    "This network is regularized and uses drops outs. \n",
    "\n",
    "The learning rate decays exponentially\n",
    "\n",
    "Activation retention probability (1-Dropout probability) increases from 0.5  in steps of 0.1\n",
    "\n",
    "Input layer = image_size * image_size\n",
    "\n",
    "Layer-1 = 1024 units\n",
    "\n",
    "Layer-2 = 512 units\n",
    "\n",
    "Layer-3 = 256 units\n",
    "\n",
    "output = num_label units\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "input_size = image_size * image_size\n",
    "num_nodes1 = 1024\n",
    "num_nodes2 = 512\n",
    "num_nodes3 = 256\n",
    "num_nodes4 = 128\n",
    "beta = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_data = tf.placeholder(tf.float32,shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_data = tf.constant(valid_dataset)\n",
    "    tf_test_data = tf.constant(test_dataset)\n",
    "    global_step = tf.Variable(0)\n",
    "    \n",
    "    beta = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Variables.\n",
    "    weights1 = tf.Variable(tf.truncated_normal([input_size, num_nodes1], stddev=np.sqrt(2.0 / input_size)))\n",
    "    biases1 = tf.Variable(tf.zeros([num_nodes1]))\n",
    "    \n",
    "    weights2 = tf.Variable(tf.truncated_normal([num_nodes1, num_nodes2], stddev=np.sqrt(2.0 / num_nodes1)))\n",
    "    biases2 = tf.Variable(tf.zeros([num_nodes2]))\n",
    "    \n",
    "    weights3 = tf.Variable(tf.truncated_normal([num_nodes2, num_nodes3], stddev=np.sqrt(2.0 / num_nodes2)))\n",
    "    biases3 = tf.Variable(tf.zeros([num_nodes3]))\n",
    "    \n",
    "    weights4 = tf.Variable(tf.truncated_normal([num_nodes3, num_nodes4], stddev=np.sqrt(2.0 / num_nodes3)))\n",
    "    biases4 = tf.Variable(tf.zeros([num_nodes4]))\n",
    "    \n",
    "    weights5 = tf.Variable(tf.truncated_normal([num_nodes4, num_labels], stddev=np.sqrt(2.0 / num_nodes4)))\n",
    "    biases5 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Training computation.\n",
    "    layer1 = tf.nn.relu(tf.matmul(tf_train_data, weights1) + biases1)\n",
    "    drop_layer1 = tf.nn.dropout(layer1, 0.5)\n",
    "    \n",
    "    layer2 = tf.nn.relu(tf.matmul(drop_layer1, weights2) + biases2)\n",
    "    drop_layer2 = tf.nn.dropout(layer2, 0.5)\n",
    "    \n",
    "    layer3 = tf.nn.relu(tf.matmul(drop_layer2, weights3) + biases3)\n",
    "    drop_layer3 = tf.nn.dropout(layer3, 0.5)\n",
    "    \n",
    "    layer4 = tf.nn.relu(tf.matmul(drop_layer3, weights4) + biases4)\n",
    "    drop_layer4 = tf.nn.dropout(layer4, 0.5)\n",
    "    \n",
    "    logit5 = tf.matmul(drop_layer4, weights5) + biases5\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logit5) +\n",
    "                         beta*(tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3) + \n",
    "                               tf.nn.l2_loss(weights4) + tf.nn.l2_loss(weights5)))\n",
    "    \n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.65, staircase=True)\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_preds = tf.nn.softmax(logit5)\n",
    "    \n",
    "    layer1_test = tf.nn.relu(tf.matmul(tf_test_data, weights1) + biases1)\n",
    "    layer2_test = tf.nn.relu(tf.matmul(layer1_test, weights2) + biases2)\n",
    "    layer3_test = tf.nn.relu(tf.matmul(layer2_test, weights3) + biases3)\n",
    "    layer4_test = tf.nn.relu(tf.matmul(layer3_test, weights4) + biases4)\n",
    "    test_preds = tf.nn.softmax(tf.matmul(layer4_test, weights5) + biases5)\n",
    "    \n",
    "    layer1_valid = tf.nn.relu(tf.matmul(tf_valid_data, weights1) + biases1)\n",
    "    layer2_valid = tf.nn.relu(tf.matmul(layer1_valid, weights2) + biases2)\n",
    "    layer3_valid = tf.nn.relu(tf.matmul(layer2_valid, weights3) + biases3)\n",
    "    layer4_valid = tf.nn.relu(tf.matmul(layer3_valid, weights4) + biases4)\n",
    "    valid_preds = tf.nn.softmax(tf.matmul(layer4_valid, weights5) + biases5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================\n",
      "Minibatch loss at step 0: 4.137491\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 11.3%\n",
      "Test accuracy: 11.5%\n",
      "=========================================\n",
      "Minibatch loss at step 300: 1.829859\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 82.9%\n",
      "Test accuracy: 90.0%\n",
      "=========================================\n",
      "Minibatch loss at step 600: 1.590990\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 84.3%\n",
      "Test accuracy: 90.8%\n",
      "=========================================\n",
      "Minibatch loss at step 900: 1.367956\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 85.2%\n",
      "Test accuracy: 92.0%\n",
      "=========================================\n",
      "Minibatch loss at step 1200: 1.015932\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 85.5%\n",
      "Test accuracy: 92.0%\n",
      "=========================================\n",
      "Minibatch loss at step 1500: 0.900927\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 86.1%\n",
      "Test accuracy: 92.7%\n",
      "=========================================\n",
      "Minibatch loss at step 1800: 0.966187\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.4%\n",
      "Test accuracy: 92.6%\n",
      "=========================================\n",
      "Minibatch loss at step 2100: 0.804079\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.0%\n",
      "Test accuracy: 93.1%\n",
      "=========================================\n",
      "Minibatch loss at step 2400: 0.846378\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 87.0%\n",
      "Test accuracy: 93.3%\n",
      "=========================================\n",
      "Minibatch loss at step 2700: 0.825121\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 87.0%\n",
      "Test accuracy: 93.6%\n",
      "=========================================\n",
      "Minibatch loss at step 3000: 0.781457\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 87.3%\n",
      "Test accuracy: 93.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size)  % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        tf_dict = {tf_train_data : batch_data, \n",
    "                   tf_train_labels : batch_labels, \n",
    "                   beta : 1e-3}\n",
    "        \n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_preds], feed_dict = tf_dict)\n",
    "        \n",
    "        if (step % 300 == 0):\n",
    "            print(\"=========================================\")\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_preds.eval(), valid_labels))\n",
    "            print(\"Test accuracy: %.1f%%\" % accuracy(test_preds.eval(), test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
